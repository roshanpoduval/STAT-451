{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 (40 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 451: Intro to Machine Learning (Fall 2020)  \n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n",
    "\n",
    "Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat479-fs2019/\n",
    "\n",
    "---\n",
    "\n",
    "**Due**: Nov 2, (5:00 pm).\n",
    "\n",
    "**How to submit**\n",
    "\n",
    "As mentioned in the lecture, you need to send the `.ipynb` file with your answers plus an `.html` file, which will serve as a backup for us in case the `.ipynb` file cannot be opened on my or the TA's computer. In addition, you may also export the notebook as PDF and upload it as well.\n",
    "\n",
    "The homework solution should be uploaded on Canvas. You can submit it as often as you like before the deadline.\n",
    "\n",
    "Note that there are 13 tasks, and the HW is worth 78 pts in total (13*6pts=78pts).\n",
    "\n",
    "**Important**\n",
    "\n",
    "- The cells that require your code answer are marked with `\"# YOUR CODE\"` comments.\n",
    "- Note that you may use 1 or more line of code for replacing each `\"# YOUR CODE\"` comment.\n",
    "\n",
    "For example, imagine there is a question asking you to implement a threshold function that should return 1 if the input `x` is greater than 0.5 and otherwise. This could appear as follows in the the exercise:\n",
    "\n",
    "```python\n",
    "def threshold_func(x):\n",
    "    # YOUR CODE\n",
    "```\n",
    "\n",
    "A valid answer could be\n",
    "\n",
    "```python\n",
    "def threshold_func(x):\n",
    "    if x > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "```\n",
    "\n",
    "Another valid solution could be\n",
    "\n",
    "```python\n",
    "def threshold_func(x):\n",
    "    return int(x > 0.5)\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roshan Poduval \n",
      "last updated: 2020-10-29 \n",
      "\n",
      "CPython 3.8.3\n",
      "IPython 7.16.1\n",
      "\n",
      "numpy 1.18.5\n",
      "scipy 1.5.0\n",
      "matplotlib 3.2.2\n",
      "sklearn 0.23.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark  -d -u -a \"Roshan Poduval\" -v -p numpy,scipy,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Implementing a \"CART\" Decision Tree from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part of the homework, you are going to implement the CART decision tree algorithm we discussed in class. This decision tree algorithm will construct a binary decision tree based on maximizing Information Gain using the Gini Impurity measure on continuous features.\n",
    "\n",
    "\n",
    "Implementing machine learning algorithms from scratch is a very important skill, and this homework will provide exercises that will help you to develop this skill. Even if you are interested in the more theoretical aspects of machine learning, being comfortable with implementing and trying out algorithms is vital for doing research, since even the more theoretical papers in machine learning are usually accompanied by experiments or simulations to a) verify results and b) to compare algorithms with the state-of-the art.\n",
    "\n",
    "Since many students are not expert Python programmers (yet), I will provide partial solutions to the homework tasks such that you have a framework or guide to implement the solutions. Areas that you need to fill in will be marked with comments (e.g., `# YOUR CODE`). For these partial solutions, I first implemented the functions myself, and then I deleted parts you need to fill in by these comments. However, note that you can, of course, use more or fewer lines of code than I did. In other words, all that matter is that the function you write can create the same outputs as the ones I provide. How many lines of code you need to implement that function, and how efficient it is, does not matter here. The expected outputs for the respective functions will be provided for most functions so that you can double-check your solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Splitting a node (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to implement a function that splits a dataset along a feature axis into sub-datasets. For this, we assume that the feature values are continuous (we are expecting NumPy float arrays). If the input is a NumPy integer array, we could convert it into a float array via \n",
    "\n",
    "    float_array = integer_array.astype(np.float64)\n",
    "\n",
    "To provide an intuitive example of how the splitting function should work, suppose you are given the following NumPy array with four feature values, feature values 0-3:\n",
    "\n",
    "    np.array([0.0, 1.0, 4.0, 1.0, 0.0, 3.0, 1.0, 0.0, 1.0, 2.0])\n",
    "    \n",
    "The function you are going to implement should return a dictionary, where the dictionary key stores the information about which data point goes to the left child note and which data point goes to the right child node after applying a threshold for splitting.\n",
    "\n",
    "For example, if we were to use a `split` function on the array shown above with a theshold $t=2.5$, the split function should return the following dictionary:\n",
    "\n",
    "\n",
    "    {\n",
    "     'left': array([0, 1, 3, 4, 6, 7, 8, 9]),   # smaller than or equal to threshold\n",
    "     'right': array([2, 5])                     # larger than threshold'\n",
    "     'threshold': 2.5                           # threshold for splitting, e.g., 2.5 means <= 2.5\n",
    "     }\n",
    "     \n",
    "Note that we also store a \"threshold\" key here to keep track of what value we used for the split -- we will need this later.\n",
    "\n",
    "Now it's your turn to implement the split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL (4 pts)\n",
    "\n",
    "\n",
    "\n",
    "def split(array, t):\n",
    "    \"\"\"\n",
    "    Function that splits a feature based on a threshold.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array : NumPy array, type float, shape=(num_examples,)\n",
    "      A NumPy array containing feature values (float values).\n",
    "      \n",
    "    t : float\n",
    "      A threshold parameter for dividing the examples into\n",
    "      a left and a right child node.\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    d : dictionary of the split\n",
    "      A dictionary that has 3 keys, 'left', 'right', 'threshold'.\n",
    "      The 'threshold' simply references the threshold t we provided\n",
    "      as function argument. The 'left' child node is an integer array\n",
    "      containing the indices of the examples corresponding to feature\n",
    "      values with value <= t. The 'right' child node is an integer array\n",
    "      stores the indices of the examples for which the feature value > t.\n",
    "    \"\"\"\n",
    "    left = np.where(array<=t)\n",
    "    right = np.where(array>t)\n",
    "    d = {'left': left, 'right': right, 'threshold': t}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'left': array([0, 1, 3, 4, 6, 7, 8, 9]), 'right': array([2, 5]), 'threshold': 2.5}\n",
      "{'left': array([0, 1, 3, 4, 6, 7, 8]), 'right': array([2, 5, 9]), 'threshold': 1.5}\n",
      "{'left': array([], dtype=int64), 'right': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'threshold': -0.5}\n",
      "{'left': array([0, 1, 3, 4, 6, 7, 8]), 'right': array([2, 5, 9]), 'threshold': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "ary = np.array([0.0, 1.0, 4.0, 1.0, 0.0, 3.0, 1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "print(split(ary, t=2.5))\n",
    "\n",
    "print(split(ary, t=1.5))\n",
    "\n",
    "print(split(ary, t=-0.5))\n",
    "\n",
    "print(split(ary, t=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2) Implement a function to compute the Gini Impurity (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the splitting function, the next step is to implement a criterion function so that we can compare splits on different features. I.e., we use this criterion function to decide which feature is the best feature to split for growing the decision tree at each node. As discussed in class, our splitting criterion will be Information Gain. However, before we implement an Information Gain function, we need to implement a function that computes the Gini Impurity at each node, which we need to compute Information Gain. For your reference, we defined Gini Impurity as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G(p) = 1 - \\sum_i (p_i)^2$$\n",
    "\n",
    "where you can think of $p_i$ as the proportion of examples with class label $i$ at a given node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL  (4 pts)\n",
    "\n",
    "\n",
    "def gini(array):\n",
    "    \"\"\"\n",
    "    Function that computes the Gini Impurity.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    array : NumPy array, type int, shape=(num_examples,)\n",
    "      A NumPy array containing integers representing class\n",
    "      labels.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Gini impurity (float value).\n",
    "    \n",
    "    Example\n",
    "    ----------\n",
    "    >>> gini(np.array([0, 0, 1, 1]))\n",
    "    0.5\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(array) == 0:\n",
    "        return 0\n",
    "    \n",
    "    n = len(array)\n",
    "    gini = 1\n",
    "    for i in range(np.max(np.unique(array)) + 1):\n",
    "        gini = gini - (sum(array==i)/n)**2\n",
    "        \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIP: To check your solution, try out the `gini` function on some example arrays. Note that the Gini impurity is maximum (0.5) if the classes are uniformly distributed; it is minimum if the array contains labels from only one single class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.0\n",
      "0.1653\n",
      "0.0\n",
      "0.6173\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "print(round(gini(np.array([0, 1, 0, 1, 1, 0])), 4))\n",
    "print(round(gini(np.array([1, 2])), 4))\n",
    "print(round(gini(np.array([1, 1])), 4))\n",
    "print(round(gini(np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), 4))\n",
    "print(round(gini(np.array([0, 0, 0])), 4))\n",
    "print(round(gini(np.array([1, 1, 1, 0, 1, 4, 4, 2, 1])), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Implement Information Gain (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a working solution for the `entropy` function, the next step is to compute the Information Gain. For your reference, information gain is computed as\n",
    "\n",
    "$$GAIN(\\mathcal{D}, x_j) = H(\\mathcal{D}) - \\sum_{v \\in Values(x_j)} \\frac{|\\mathcal{D}_v|}{|\\mathcal{D}|} H(\\mathcal{D}_v).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL (4 pts)\n",
    "\n",
    "\n",
    "def information_gain(x_array, y_array, split_dict):\n",
    "    \"\"\"\n",
    "    Function to compute information gain.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    \n",
    "    x_array : NumPy array, shape=(num_examples)\n",
    "      NumPy array containing the continuous feature\n",
    "      values of a given feature variable x.\n",
    "      \n",
    "    y_array : NumPy array, shape=(num_examples)\n",
    "      NumPy array containing the integer class labels\n",
    "      corresponding to the training examples.\n",
    "      \n",
    "    split_dict : dictionary\n",
    "      A dictionary created by the `split` function, which\n",
    "      contains the indices for the left and right child node.\n",
    "      \n",
    "    Returns\n",
    "    ---------\n",
    "    \n",
    "    Information gain for the given split in `split_dict`.\n",
    "    \n",
    "    \"\"\"\n",
    "    parent_n = len(x_array)\n",
    "    parent_entropy = gini(y_array)\n",
    "    \n",
    "    for child in ('left', 'right'):\n",
    "        # TIP: freq := |D_v| / |D|\n",
    "        freq = len(y_array[split_dict[child]])/parent_n\n",
    "        child_entropy = gini(y_array[split_dict[child]])\n",
    "        child_entropy = gini(y_array[split_dict[child]])\n",
    "        parent_entropy -= freq * child_entropy\n",
    "        \n",
    "    return parent_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `information_gain` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004999999999999977\n",
      "0.003809523809523735\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "x_ary = np.array([0.0, 1.0, 4.0, 1.0, 0.0, 3.0, 1.0, 0.0, 1.0, 2.0])\n",
    "y_ary = np.array([0, 1, 1, 0, 0, 0, 1, 1, 0, 0])\n",
    "\n",
    "split_dict_1 = split(ary, t=2.5)\n",
    "print(information_gain(x_array=x_ary, \n",
    "                       y_array=y_ary,\n",
    "                       split_dict=split_dict_1))\n",
    "      \n",
    "split_dict_2 = split(ary, t=1.5)\n",
    "print(information_gain(x_array=x_ary, \n",
    "                       y_array=y_ary,\n",
    "                       split_dict=split_dict_2))\n",
    "\n",
    "split_dict_3 = split(ary, t=-1.5)\n",
    "print(information_gain(x_array=x_ary, \n",
    "                       y_array=y_ary,\n",
    "                       split_dict=split_dict_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Creating different splitting thresholds (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should have almost all the main components that we need for implementing the CART decision tree algorithm: a `split` function, a `gini` function, and an `information_gain` function based on the `gini` function. However, since we are working with continuous feature variables, we need to find a good threshold $t$ on the number line of each feature, which we can use with our function `split`. \n",
    "\n",
    "\n",
    "For simplicity, we are going to implement a function that creates different thresholds based on the values found in a given feature variable. More precisely, we are going to implement a function `get_thresholds` that returns the lowest and highest feature value in a feature value array, plus the midpoint between each adjacent pairs of features (assuming the feature variable is sorted). \n",
    "\n",
    "\n",
    "For example, if a feature array consists of the values\n",
    "\n",
    "    [0.1, 1.2, 2.4, 2.5, 2.7, 3.3, 3.7]\n",
    "    \n",
    "the returned thresholds should be\n",
    "\n",
    "    [0.1, 0.1+1.2/2, 1.2+2.4/2, 2.4+2.5/2, 2.5+2.7/2, 2.7+3.3/2, 3.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL  (4 pts)\n",
    "\n",
    "def get_thresholds(array):\n",
    "    \"\"\"\n",
    "    Get thresholds from a feature array.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    array : NumPy array, type float, shape=(num_examples,)\n",
    "      Array with feature values.\n",
    "      \n",
    "    Returns\n",
    "    -----------\n",
    "    NumPy float array containing thresholds.\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE (multiple lines)\n",
    "    n = len(array)\n",
    "    array = np.sort(array)\n",
    "    output = np.zeros(n+1)\n",
    "    for i in range(n):\n",
    "        if i == 0:\n",
    "            output[i] = array[i]\n",
    "        else:\n",
    "            output[i] = (array[i-1] + array[i]) / 2.0\n",
    "            \n",
    "    output[n] = array[n-1]\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1  0.65 1.8  2.45 2.6  3.   3.5  3.7 ]\n",
      "[0.1  0.65 1.8  2.45 2.6  3.   3.5  3.7 ]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "a = np.array([0.1, 1.2, 2.4, 2.5, 2.7, 3.3, 3.7])\n",
    "print(get_thresholds(a))\n",
    "\n",
    "b = np.array([3.7, 2.4, 1.2, 2.5, 3.3, 2.7, 0.1])\n",
    "print(get_thresholds(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5) Selecting the best splitting threshold  (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we implemented a function `get_thresholds` to create different splitting thresholds for a given feature. In this section, we are now implementing a function that selects the best threshold (the threshold that results in the largest information gain) from the array returned by `get_thresholds` by combining the \n",
    "\n",
    "- `get_thresholds`\n",
    "- `split`\n",
    "- `information_gain`\n",
    "\n",
    "functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL  (4 pts)\n",
    "\n",
    "def get_best_threshold(x_array, y_array):\n",
    "    \"\"\"\n",
    "    Function to obtain the best threshold\n",
    "    based on maximizing information gain.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    x_array : NumPy array, type float, shape=(num_examples,)\n",
    "      Feature array containing the feature values of a feature\n",
    "      for each training example.\n",
    "    y_array : NumPy array, type int, shape=(num_examples,)\n",
    "      NumPy array containing the class labels for each\n",
    "      training example.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A float representing the best threshold to split the given\n",
    "    feature variable on.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_thresholds = get_thresholds(x_array)\n",
    "    info_gains = np.zeros(all_thresholds.shape[0])\n",
    "\n",
    "    for idx, t in enumerate(all_thresholds):\n",
    "    \n",
    "        split_dict_t = split(x_array, t=t)\n",
    "        ig = information_gain(x_array, y_array, split_dict_t)\n",
    "\n",
    "        info_gains[idx] = ig\n",
    "        \n",
    "    best_idx = np.argmax(info_gains)\n",
    "    best_threshold = all_thresholds[best_idx]\n",
    "    \n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n",
      "3.5\n"
     ]
    }
   ],
   "source": [
    "x_ary = np.array([0.0, 1.0, 4.0, 1.0, 0.0, 3.0, 1.0, 0.0, 1.0, 2.0])\n",
    "y_ary = np.array([0, 1, 1, 0, 0, 0, 1, 1, 0, 0])\n",
    "\n",
    "print(get_best_threshold(x_array=x_ary, \n",
    "                         y_array=y_ary))\n",
    "\n",
    "x_ary = np.array([0.0, 3.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 4.0, 1.0,])\n",
    "y_ary = np.array([0, 0, 1, 1, 0, 0, 0, 1, 1, 0])\n",
    "\n",
    "print(get_best_threshold(x_array=x_ary, \n",
    "                         y_array=y_ary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6) Decision Tree Splitting  (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task is to combine all the previously developed functions to recursively split a dataset on its different features to construct a decision tree that separates the examples from different classes well. We will call this function `make_tree`. \n",
    "\n",
    "For simplicity, the decision tree returned by the `make_tree` function will be represented by a Python dictionary. To illustrate this, consider the following dataset consisting of 6 training examples (class labels are 0 or 1) and 2 feature variables $X_0$ and $X_1$:\n",
    "\n",
    "```\n",
    "Inputs:\n",
    " [[0. 0.]\n",
    "  [0. 1.]\n",
    "  [1. 0.]\n",
    "  [1. 1.]\n",
    "  [2. 0.]\n",
    "  [2. 1.]]\n",
    "\n",
    "Labels:\n",
    " [0 1 0 1 1 1]\n",
    "```\n",
    " \n",
    "Based on this dataset with 6 training examples and two features, the resulting decision tree in form of the Python dictionary should look like as follows:\n",
    "\n",
    "\n",
    "\n",
    "You should return a dictionary with the following form:\n",
    "\n",
    "```\n",
    " {'X_1 <= 0.000000': {'X_0 <= 1.500000': array([0, 0]), \n",
    "                      'X_0 > 1.500000': array([1])\n",
    "                     }, \n",
    "                      \n",
    "  'X_1 > 0.000000': array([1, 1, 1])                 \n",
    " }\n",
    " ```\n",
    " \n",
    "Let me further illustrate what the different parts of the dictionary mean. Here, the `'X_1'` in `'X_1 <= 0'` refers feature 2 (the first column of the NumPy array; remember that Python starts the index at 0, in contrast to R). \n",
    "\n",
    "- 'X_1 <= 0': For training examples stored in this node where the 2nd feature is less than or equal to 0.\n",
    "- 'X_1 > 0': For training examples stored in this node where the 2nd feature is larger than 0.\n",
    "\n",
    "The \"array\" is a NumPy array that stores the class labels of the training examples at that node. In the case of `'X_1 <= 0'` we actually store actually a sub-dictionary, because this node can be split further into 2 child nodes with `'X_0 <= 1.500000'` and `'X_0 > 1.500000'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL (4 pts)\n",
    "\n",
    "def make_tree(X, y):\n",
    "    \"\"\"\n",
    "    A recursive function for building a binary decision tree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : NumPy array, type float, shape=(num_examples, num_features)\n",
    "      A design matrix representing the feature values.\n",
    "      \n",
    "    y : NumPy array, type int, shape=(num_examples,)\n",
    "      NumPy array containing the class labels corresponding to the training examples.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Dictionary representation of the decision tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return class label array if node is empty or pure (1 example in leaf node)\n",
    "    if np.all(y == y[0]):\n",
    "        return y\n",
    "    \n",
    "    # Select the best threshold for each feature\n",
    "    n_features = X.shape[1]\n",
    "    thresholds = np.zeros(n_features) # YOUR CODE\n",
    "    for i in range(n_features):\n",
    "        thresholds[i] = get_best_threshold(X.T[i][:], y)\n",
    "\n",
    "    # Compute information gain for each feature based on the best threshold for each feature\n",
    "    \n",
    "    gains = np.zeros(X.shape[1])\n",
    "    split_dicts = []\n",
    "\n",
    "    for idx, (feature, threshold) in enumerate(zip(X.T, thresholds)):\n",
    "        split_dict = split(feature, threshold)\n",
    "        split_dicts.append(split_dict)\n",
    "        ig = information_gain(feature, y, split_dict)\n",
    "        gains[idx] = ig\n",
    "\n",
    "    # Early stopping if there is no information gain\n",
    "    if (gains <= 1e-05).all():\n",
    "        return y\n",
    "    \n",
    "    # Else, get best feature\n",
    "    best_feature_idx = np.argmax(gains) # YOUR CODE\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    subset_dict = split_dicts[best_feature_idx]\n",
    "    \n",
    "    for node in ('left', 'right'):\n",
    "        child_y_subset = y[subset_dict[node]]\n",
    "        child_X_subset = X[subset_dict[node]]\n",
    "            \n",
    "        if node == 'left':\n",
    "            results[\"X_%d <= %f\" % (best_feature_idx, subset_dict['threshold'])] = \\\n",
    "                    make_tree(child_X_subset, child_y_subset)\n",
    "            \n",
    "        else:\n",
    "            results[\"X_%d > %f\" % (best_feature_idx, subset_dict['threshold'])] = \\\n",
    "                    make_tree(child_X_subset, child_y_subset)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `make_tree` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [2. 0.]\n",
      " [2. 1.]]\n",
      "\n",
      "Labels:\n",
      " [0 1 0 1 1 1]\n",
      "\n",
      "Decision tree:\n",
      " {'X_1 <= 0.000000': {'X_0 <= 1.500000': array([0, 0]), 'X_0 > 1.500000': array([1])}, 'X_1 > 0.000000': array([1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "x1 = np.array([0., 0., 1., 1., 2., 2.])\n",
    "x2 = np.array([0., 1., 0., 1., 0., 1.])\n",
    "X = np.array( [x1, x2]).T\n",
    "y = np.array( [0,  1,  0,  1,  1,  1])\n",
    "\n",
    "print('Inputs:\\n', X)\n",
    "print('\\nLabels:\\n', y)\n",
    "\n",
    "print('\\nDecision tree:\\n', make_tree(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7) Building a Decision Tree API  (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of this part of the homework is now to write an API around our decision tree code so that we can use is for making predictions. Here, we will use the common convention, established by scikit-learn, to implement the decision tree as a Python class with \n",
    "\n",
    "- a `fit` method that learns the decision tree model from a training set via the `make_tree` function we already implemented;\n",
    "- a `predict` method to predict the class labels of training examples or any unseen data points.\n",
    "\n",
    "For making predictions, since not all leaf nodes are guaranteed to be single training examples, we will use a majority voting function to predict the class label as discussed in class. I already implemented a `_traverse` method, which will recursively traverse a decision tree dictionary that is produced by the `make_tree` function.\n",
    "\n",
    "Note that for simplicity, the `predict` method will only be able to accept one data point at a time (instead of a collection of data points). Hence `x` is a vector of size $\\mathbb{R}^m$, where $m$ is the number of features. I use capital letters `X` to denote a matrix of size $\\mathbb{R}^{n\\times m}$, where $n$ is the number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL (4 pts)\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "class CARTDecisionTreeClassifer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.splits_ = make_tree(X, y)\n",
    "        \n",
    "    def _majority_vote(self, label_array):\n",
    "        \"\"\" Returns a class label by majority voting\n",
    "            on an array label_array\n",
    "        \"\"\"\n",
    "        return stats.mode(label_array)[0][0] # YOUR CODE\n",
    "\n",
    "    def _traverse(self, x, d):\n",
    "        if isinstance(d, np.ndarray):\n",
    "            return d\n",
    "        for key in d:\n",
    "            \n",
    "            \n",
    "            if '<=' in key:\n",
    "                name, value = key.split(' <= ')\n",
    "                feature_idx = int(name.split('_')[-1])\n",
    "                value = float(value)\n",
    "                if x[feature_idx] <= value:\n",
    "                    return self._traverse(x, d[key])\n",
    "            else:\n",
    "                # YOUR CODE\n",
    "                name, value = key.split(' > ')\n",
    "                feature_idx = int(name.split('_')[-1])\n",
    "                value = float(value)\n",
    "                if x[feature_idx] > value:\n",
    "                    return self._traverse(x, d[key])\n",
    "\n",
    "    def predict(self, x):\n",
    "        label_array = self._traverse(x, self.splits_)\n",
    "        return self._majority_vote(label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `make_tree` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "tree = CARTDecisionTreeClassifer()\n",
    "tree.fit(X, y)\n",
    "\n",
    "print(tree.predict(np.array([0., 0.])))\n",
    "print(tree.predict(np.array([0., 1.])))\n",
    "print(tree.predict(np.array([1., 0.])))\n",
    "print(tree.predict(np.array([1., 0.])))\n",
    "print(tree.predict(np.array([1., 1.])))\n",
    "print(tree.predict(np.array([2., 0.])))\n",
    "print(tree.predict(np.array([2., 1.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part of this homework, you will be combining multiple decision trees to a bagging classifier. This time, we will be using the decision tree algorithm implemented in scikit-learn (which is some variant of the CART algorithm for binary splits, as implemented earlier and discussed in class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bootrapping  (2x 4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you remember, bagging relies on bootstrap sampling. So, as a first step, your task is to implement a function for generating bootstrap samples. In this exercise, for simplicity, we will perform the computations based on the Iris dataset.\n",
    "\n",
    "On an interesting side note, scikit-learn recently updated their version of the Iris dataset since it was discovered that the Iris version hosted on the UCI machine learning repository (https://archive.ics.uci.edu/ml/datasets/Iris/) has two data points that are different from R. Fisher's original paper (Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).) and changed it in their most recent version. Since most students may not have the latest scikit-learn version installed, we will be working with the Iris dataset that is deposited on UCI, which has become quite the standard in the Python machine learning community for benchmarking algorithms. Instead of manually downloading it, we will be fetching it through the `mlxtend` (http://rasbt.github.io/mlxtend/) library that you installed in the last homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 150\n",
      "Number of features: 4\n",
      "Unique class labels: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "from mlxtend.data import iris_data\n",
    "X, y = iris_data()\n",
    "\n",
    "print('Number of examples:', X.shape[0])\n",
    "print('Number of features:', X.shape[1])\n",
    "print('Unique class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use scikit-learn's `train_test_split` function to divide the dataset into a training and a test set.\n",
    "\n",
    "- The test set should contain 45 examples, and the training set should contain 105 examples.\n",
    "- To ensure reproducible results, use `123` as a random seed.\n",
    "- Perform a stratified split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 105\n",
      "Number of test examples: 45\n"
     ]
    }
   ],
   "source": [
    "# EDIT THIS CELL (4 pts)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=45, # YOUR CODE\n",
    "                                                    train_size=105,\n",
    "                                                    random_state=123,\n",
    "                                                    stratify=y\n",
    "                                                   )\n",
    "\n",
    "print('Number of training examples:', X_train.shape[0])\n",
    "print('Number of test examples:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are implementing a function to generate bootstrap samples of the training set. In particular, we will perform the bootstrapping as follows:\n",
    "\n",
    "- Create an index array with values 0, ..., 104.\n",
    "- Draw a random sample (with replacement) from this index array using the `choice` method of a NumPy `RandomState` object that is passed to the function as `rng`. \n",
    "- Select training examples from the X array and labels from the y array using the new sample of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL  (4 pts)\n",
    "\n",
    "def draw_bootstrap_sample(rng, X, y):\n",
    "    sample_indices = np.arange(X.shape[0])\n",
    "    bootstrap_indices = rng.choice(sample_indices, 105) # YOUR CODE\n",
    "    return X[bootstrap_indices], y[bootstrap_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `draw_bootstrap_sample` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training inputs from bootstrap round: 105\n",
      "Number of training labels from bootstrap round: 105\n",
      "Labels:\n",
      " [0 0 1 0 0 1 2 0 2 1 0 0 2 1 1 1 1 2 1 1 2 0 2 1 2 1 1 1 0 1 0 0 1 2 0 0 0\n",
      " 0 2 1 1 2 1 2 1 1 2 1 2 0 1 1 2 2 1 0 1 0 2 2 0 1 0 2 0 0 0 0 1 2 0 0 1 0\n",
      " 1 1 0 1 1 2 2 0 2 0 2 0 1 1 2 2 0 2 2 2 0 1 0 1 2 2 2 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "\n",
    "print('Number of training inputs from bootstrap round:', X_boot.shape[0])\n",
    "print('Number of training labels from bootstrap round:', y_boot.shape[0])\n",
    "print('Labels:\\n', y_boot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Baggging classifier from decision trees (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement a Bagging algorithm based on the `DecisionTreeClassifier`. I provided a partial solution for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL (4 pts)\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "class BaggingClassifier(object):\n",
    "    \n",
    "    def __init__(self, num_trees=10, random_state=123):\n",
    "        self.num_trees = num_trees\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.trees_ = [DecisionTreeClassifier(random_state=self.rng) for i in range(self.num_trees)]\n",
    "        for i in range(self.num_trees):\n",
    "            X_boot, y_boot = draw_bootstrap_sample(self.rng, X, y)\n",
    "            # YOUR CODE to\n",
    "            # fit the trees in self.trees_ on the bootstrap samples\n",
    "            self.trees_[i] = self.trees_[i].fit(X_boot, y_boot) \n",
    "            \n",
    "        \n",
    "    def predict(self, X):\n",
    "        ary = np.zeros((X.shape[0], len(self.trees_)), dtype=np.int)\n",
    "        for i in range(len(self.trees_)):\n",
    "            ary[:, i] = self.trees_[i].predict(X)\n",
    "\n",
    "        maj = np.apply_along_axis(lambda x:\n",
    "                                  np.argmax(np.bincount(x)),\n",
    "                                            axis=1,\n",
    "                                            arr=ary)\n",
    "        return maj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `BaggingClassifier()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Tree Accuracies:\n",
      "88.9%\n",
      "93.3%\n",
      "97.8%\n",
      "93.3%\n",
      "93.3%\n",
      "93.3%\n",
      "91.1%\n",
      "97.8%\n",
      "97.8%\n",
      "97.8%\n",
      "\n",
      "Bagging Test Accuracy: 97.8%\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "model = BaggingClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print('Individual Tree Accuracies:')\n",
    "for tree in model.trees_:\n",
    "    predictions = tree.predict(X_test) \n",
    "    print('%.1f%%' % ((predictions == y_test).sum() / X_test.shape[0] * 100))\n",
    "\n",
    "print('\\nBagging Test Accuracy: %.1f%%' % ((predictions == y_test).sum() / X_test.shape[0] * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
